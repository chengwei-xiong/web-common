## 分布式基础
概述：本文介绍在分布式系统基本的理论基础，针对一些关键的机制做了详细的解释并用具体的商业产品进行解释。

### 基本概念

* 节点Node：独立的程序服务，可能是同一个进程下的服务，也可能是同台机器的不同进程
* 通信： 通过网络进行通信
* 存储：数据的持久化，就是数据写入磁盘或者SSD中
* 异常：机器宕机，网络异常，存储数据异常，请求的三状态成功，超时，异常错误

### 数据/节点的分布（Sharding分片）

* Hash方式
* 按数据范围分布
* 按数据量分布
* 一致性Hash
* 本地化计算

### 高可用（Replication副本）
概述：在分布式系统当中，为了提供高可用的服务，避免网络分化，机器宕机等导致的服务/数据不可用的情况发生，一般都会为服务/数据提供多副本的机制，副本是指在分布式系统中为数据/服务提供的重复性，对于数据副本是指在不同的节点上持久化相同一份数据，当出现某一个几点上存储的数据发生丢失时，可以从其他的副本上读取数据。数据副本是分布式系统解决数据丢失异常的唯一手段；另一类服务副本是指在大规模的分布式无状态的计算模型下，在不同的节点部署了相同的服务。

GFS系统中的Chunk的数个副本就是典型的数据副本模型，MapReduce系统的JobWorker则是典型的服务副本模型。

#### 副本协议

* 中心化副本控制协议
	* 主从架构的中心化副本控制协议
		* 数据更新流程
		* 数据读取方式 
		* Primary 副本的确定和切换
		* 数据同步(reconcile)
* 去中心化副本控制协议

* 实践 
	这里简要分析几种典型的分布式系统在副本控制协议方面的特点。工程中大量的副本控制协议 都是primary-secondary 型协议。从下面这些具体的分布式系统中不难看出,Primary-secondary 型副本控制协议虽然简单,但使用却极其广泛。
* GFS中的Primary-Secondary协议	GFS 系统的副本控制协议是典型的 Primary-Secondary 型协议,Primary 副本由 Master 指定, Primary 副本决定并发更新操作的顺序。虽然在 GFS 中,更新操作的数据由客户端提交,并在各个副本之间流式的传输,及由上一个副本传递到下一个副本,每个副本都即接受其他副本的更新,也向下更新另一个副本,但是数据的更新过程完全是由 primary 控制的,所以也可以认为数据是由 primary 副本同步到 secondary 副本的。

* PNUTS中的Primary-Secondary协议
	PNUTS 的副本控制协议也是典型的 primary-secondary 型协议。Primary 副本负责将更新操作向 YMB 中提交,当更新记录写入 YMB 则认为更新成功。YMB 本身是一个分布式的消息发布、订阅 系统,其具有多副本、高可用、跨地域等特性。Secondary 副本向 YMB 订阅 primary 发布的更新操 作,当收到更新操作后,secondary 副本更新本地数据。从而,数据的更新过程也完全是由 primary 副本进行控制的。
*  Niobe中的Primary-Secondary协议
	Niobe 协议又是一个典型的 primary-secondary 型协议。Niobe 协议中,primary 信息由 GSM 模块维护,更新操作由 primary 副本同步到 secondary 副本。
* Dynamo/Cassandra的去中心化副本控制协议
	Dynamo / Cassandra 使用基于一致性哈希的去中心化协议。虽然 Dynamo 尝试通过引入 Quorum 机制和 vector clock 机制解决读取数据的一致性问题,但其一致性模型依旧是一个较大的问题。由于 缺乏较好的一致性,应用在编程时的难度被大大增加了。本文在 2.4.6 中较为详细的分析了 Dynamo 的一致性模型及其优缺点。
* Chubby/Zookeeper的副本控制协议
	Chubby[13]和 Zookeeper 使用了基于 Paxos 的去中心化协议选出 primary 节点,但完成 primary 节点的选举后,这两个系统都转为中心化的副本控制协议,即由 primary 节点负责同步更新操作到 secondary 节点。本文在 2.8.6 中进一步分析这三个系统的工作原理。* Megastore的副本控制协议
	虽然都使用了 Paxos 协议,但与 Chubby 和 Zookeeper 不同的是,Megastore 中每次数据更新操 作都基于一个改进的 Paxos 协议的实例,而不是利用 paxos 协议先选出 primary 后,再转为中心化的 primary-secondary 方式[12]。另一方面,Megastore 又结合了 Primary-secondary 本文在 2.8.6 中进一 步分析 Megastore 使用 paxos 的工作原理。 

#### 副本的一致性
概述：分布式心跳通过副本控制协议，是的从系统外部读取系统内部各个副本的数据在一定的约束条件下是相同的，根据条件的约束，分为：

* 强一致性：

* 单调一致性：

* 会话一致性：

* 最终一致性：

* 弱一致性：一旦某一个更新成功，用户无法再一个确定时间内读到这次更新的值，即使在某个副本上读到新的值，也不能保证能在其他副本中读取到最新的值。

分布式的一致性相关协议：

* Lease机制

	Lease就是一把带有超时机制的分布式锁，如果没有Lease，分布式环境中的锁可能会因为锁拥有者的失败而导致死锁，有了lease，死锁会被控制在超时时间之内，也就是说Lease是由颁发者授予的在某一有效期内的承诺。

	Lease 机制依赖于有效期,这就要求颁发者和接收者的时钟是同步的。一方面,如果颁发者的 时钟比接收者的时钟慢,则当接收者认为 lease 已经过期的时候,颁发者依旧认为 lease 有效。接收 者可以用在 lease 到期前申请新的 lease 的方式解决这个问题。另一方面,如果颁发者的时钟比接收 者的时钟快,则当颁发者认为 lease 已经过期的时候,接收者依旧认为 lease 有效,颁发者可能将 lease 颁发给其他节点,造成承诺失效,影响系统的正确性。对于这种时钟不同步,实践中的通常做法是 将颁发者的有效期设置得比接收者的略大,只需大过时钟误差就可以避免对 lease 的有效性的影响。
		Lease 的有效期虽然是一个确定的时间点,当颁发者在发布 lease 时通常都是将当前时间加上一 个固定的时长从而计算出lease的有效期；工程中,常选择的 lease 时长是 10 秒级别,这是一个经过验证的经验值,实践中可以作为参考并综合选择合适的时长。

	* GFS中的Lease	GFS 中使用 Lease 确定 Chuck 的 Primary 副本。Lease 由 Master 节点颁发给 primary 副本,持有 Lease 的副本成为 primary 副本。Primary 副本控制该 chuck 的数据更新流量,确定并发更新操作在 chuck 上的执行顺序。GFS 中的 Lease 信息由 Master 在响应各个节点的 HeartBeat 时附带传递 (piggyback)。对于每一个 chuck,其上的并发更新操作的顺序在各个副本上是一致的,首先 master 选择 primary 的顺序,即颁发 Lease 的顺序,在每一任的 primary 任期内,每个 primary 决定并发更 新的顺序,从而更新操作的顺序最终全局一致。当 GFS 的 master 失去某个节点的 HeartBeat 时,只需待该节点上的primary chuck的Lease超时,就可以为这些chuck重新选择primary副本并颁发lease。
	* Chubby与Zookeeper中的Lease	Chubby 中有两处使用到 Lease 机制。我们知道 chubby 通过 paxos 协议实现去中心化的选择 primary 节点(见 2.8.6 )。一旦某个节点 获得了超过半数的节点认可,该节点成为 primary 节点,其余节点成为 secondary 节点。Secondary 节点向 primary 节点发送 lease,该 lease 的含义是:“承诺在 lease 时间内,不选举其他节点成为 primary 节点”。只要 primary 节点持有超过半数节点的 lease,那么剩余的节点就不能选举出新的 primary。

	一旦 primary 宕机,剩余的 secondary 节点由于不能向 primary 节点发送 lease,将发起新的一轮 paxos 选举,选举出新的 primary 节点。这种由 secondary 向 primary 发送 lease 的形式与 niobe 的 lease 形 式有些类似。除了 secondary 与 primary 之间的 lease,在 chubby 中,primary 节点也会向每个 client 节点颁发 lease。该 lease 的含义是用来判断 client 的死活状态,一个 client 节点只有只有合法的 lease,才能与 chubby 中的 primary 进行读写操作。一个 client 如果占有 chubby 中的一个节点锁后 lease 超时,那 么这个 client 占有的 chubby 锁会被自动释放,从而实现了利用 chubby 对节点状态进行监控的功能。 另外,chubby中client中保存有数据的cache,故此chubby的primary为cache的数据颁发cache lease, 该过程与 2.3.1 中介绍的基于 lease 的 cahce 机制完全类似。虽然相关文献上没有直接说明,但笔者 认为,chubby的cache lease与primary用于判断client死活状态的lease是可以合并为同一个lease 的,从而可以简化系统的逻辑。
	与 Chubby 不同,Zookeeper 中的 secondary 节点(在 zookeeper 中称之为 follower)并不向 primary 节点(在zookeeper中称之为leader)发送lease,zookeeper中的secondary节点如果发现没有primary 节点则发起新的 paxos 选举,只要 primary 与 secondary 工作正常,新发起的选举由于缺乏多数 secondary 的参与而不会成功。与 Chubby 类似的是,Zookeeper 的 primary 节点也会向 client 颁发 lease, lease 的时间正是 zookeeper 中的 session 时间。在 Zookeeper 中,临时节点是与 session 的生命期绑定 的,当一个 client 的 session 超时,那么这个 client 创建的临时节点会被 zookeeper 自动删除。通过监 控临时节点的状态,也可以很容易的实现对节点状态的监控。在这一点上,zookeeper 和 chubby 完 全是异曲同工。

* Paxos
* Raft
* 2PC,3PC
* Quorum
	WARO 牺牲了更新服务的可用性,最大程度的增强读服务的可用性。下面将 WARO 的条件进行松弛,从而使得可以在读写服务可用性之间做折中,得出 Quorum 机制。在 Quorum 机制下,当某次更新操作 wi 一旦在所有 N 个副本中的 W 个副本上都成功,则就称 该更新操作为“成功提交的更新操作”,称对应的数据为“成功提交的数据”。
	仅仅依赖 quorum 机制是无法保证强一致性的。因为仅有 quorum 机制时无法确 定最新已成功提交的版本号,除非将最新已提交的版本号作为元数据由特定的元数据服务器或元数 据集群管理,否则很难确定最新成功提交的版本号。
	GFS中的Quorum	GFS 使用 WARO 机制读写副本,即如果更新所有副本成功则认为更新成功,一旦更新成功, 则可以任意选择一个副本读取数据;如果更新某个副本失败,则更显失败,副本之间处于不一致的 状态。GFS 系统不保证异常状态时副本的一致性,GFS 系统需要上层应用通过 Checksum 等机制自 行判断数据是否合法。值得注意的是 GFS 中的 append 操作,一旦 append 操作某个 chunck 的副本上 失败,GFS 系统会自动新增一个 chunck 并尝试 append 操作,由于可以让新增的 chunck 在正常的机 器上创建,从而解决了由于 WARO 造成的系统可用性下降问题。进而在 GFS 中,append 操作不保 证一定在文件的结尾进行,由于在新增的 chunk 上重试 append,append 的数据可能会出现多份重复的现象,但每个 append 操作会返回用户最终成功的 offset 位置,在这个位置上,任意读取某个副本 一定可以读到写入的数据。这种在新增 chunk 上进行尝试的思路,大大增大了系统的容错能力,提高了系统可用性,是一种非常值得借鉴的设计思路。
	Dynamo中的Quorum
		Dynamo/Cassandra 是一种去中心化的分布式存储系统。Dynamo 使用 Quorum 机制来管理副本。 用户可以配置 N、R、W 的参数,并保证满足 R+W>N 的 quorum 要求。与其他系统的 Quorum 机制 类似,更新数据时,至少成功更新 W 个副本返回用户成功,读取数据时至少返回 R 个副本的数据。 然而 Dynamo 是一个没有 primary 中的去中心化系统,由于缺乏中心控制,每次更新操作都可能由 不同的副本主导,在出现并发更新、系统异常时,其副本的一致性完全无法得到保障。下面着重分析 Dynamo 在异常时副本的一致性情况。首先,Dynamo 使用一致性哈希分布数据, 理论上,即使出现一个节点异常,更新操作也可以顺着一致性哈希环的顺序找到 N 个节点完成。不 过,这里我们简化其模型,认为始终只有初始的 N 个副本,在实际中可以等效为网络异常造成用户 只能和初始的 N 个副本通信。更复杂的是,虽然可以沿哈希环找到下一个节点临时加入,但无法解 决异常节点又重新加入的问题,所以这里的这种简化模型是完全合理的。我们通过一个例子来考察 Dynamo 的一致性。例 2.4.6,在 Dynamo 系统中,N=3,R=2,W=2,初始时,数据 3 个副本(A、B、C)上的数 据一致,这里假设数据值都为 1,即(1,1,1)。某次更新操作需在原有数据的基础上增加新数据,这里假设为+1操作,该操作由副本A主导, 副本 A 成功更新自己及副本 C,由于异常,更新副本 B 失败,由于已经满足 W=2 的要求,返回用 户更新成功。此时 3 个副本上的数据分别为(2,1,2)。接着,进行新的更新操作,该操作需要在原有数据的基础上增加新数据,假设为+2 操作,假设 用户端由于异常联系副本 A 失败,联系副本 B 成功,本次更新操作由副本 B 主导,副本 B 读取本 地数据 1,完成加 2 操作后同步给其他副本,假设同步副本 C 成功,此时满足 W=2 的要求,返回用 户更新成功。此时 3 个副本上的数据分别为(2, 3, 3)。这里需要说明的是在 Dynamo 中,副本 C 必须 要接受副本 B 发过来的更新并覆盖自身数据,即使从全局角度说该更新与副本 C 上的已有数据是冲 突的,但副本 C 自身无法判断自己的数据是否有效。假如第一次副本 A 主导的更新只在副本 C 上 成功,那么此时副本 C 上的数据本身就是错误的脏数据,被副本 B 主导的这次更新覆盖也是完全应 该的。最后,用户读取数据,假设成功读取副本 A 及副本 B 上的数据,满足 R=2 的需求,用户将拿 到两个完全不一致的数据 2 与 3,Dynamo 将解决这种不一致的情况留给了用户进行。为了帮助用户解决这种不一致的情况,Dynamo 提出了一种 clock vector 的方法,该方法的思路￼￼￼就是记录数据的版本变化,以类似 MVCC(2.7 )的方式帮助用户解决数据冲突。所谓 clock vector 即记录了数据变化的路径的向量,为每个更新操作维护分配一个向量元素,记录数据的版本号及主 导该次更新的副本名字。接着例 2.4.7 来介绍 clock vector 的过程。例 2.4.8:在 Dynamo 系统中,N=3,R=2,W=2,初始时,数据 3 个副本(A、B、C)上的数 据一致,这里假设数据值都为 1,即(1,1,1),此时三个副本的 clock vector 都为空([], [], [])。某次更新操作需在原有数据的基础上增加新数据,这里假设为+1操作,该操作由副本A主导, 副本 A 成功更新自己及副本 C,返回用户更新成功。此时 3 个副本上的数据分别为(2,1,2),而 三个副本的 clock vector 为([(1, A)], [], [(1, A)]),A、C 的 clock vector 表示数据版本号为 1,更新是 有副本 A 主导的。接着,进行新的更新操作,该操作需要在原有数据的基础上增加新数据,假设为+2 操作,假设 本次更新操作由副本 B 主导,副本 B 读取本地数据 1,完成加 2 操作后同步给其他副本,假设同步 副本 C 成功,。此时 3 个副本上的数据分别为(2, 3, 3),此时三个副本的 clock vector 为([(1, A)], [(1, B)], [(1, B)])。为了说明 clock vector,这里再加入一次+3 操作,并由副本 A 主导,更新副本 A 及副本 C 成功, 此时数据为(5, 3, 5),此时三个副本的 clock vector 为([(2, A), (1, A)], [(1, B)], [(2, A), (1, A)])。最后,用户读取数据,假设成功读取副本 A 及副本 B 上的数据,得到两个完全不一致的数据 5 与 3,及这两个数据的版本信息[(2, A), (1, A)], [(1, B)]。用户可以根据自定义的策略进行合并,例如 假设用户判断出,其实这些加法操作可以合并,那么最终的数据应该是 7,又例如用户可以选择保 留一个数据例如 5 作为自己的数据。由于提供了 clock vector 信息,不一致的数据其实成为了多版本数据,用户可以通过自定义策略 选择合并这些多版本数据。Dynamo 建议可以简单的按照数据更新的时间戳进行合并,即用数据时 间戳较新的数据替代较旧的数据。如果是简单的覆盖写操作,例如设置某个用户属性,这样的策略 是有效且正确的。然而类似上例中这类并发的加法操作(例如“向购物车中增加商品”),简单的用 新数据替代旧数据的方式就是不正确的,会造成数据丢失。	Zookeeper中的Quorum
	Zookeeper使用的paxos协议本身就是利用了Quorum机制,在2.8 中有详细分析,这里不赘述。 当利用 paxos 协议外选出 primary 后,Zookeeper 的更新流量由 primary 节点控制,每次更新操作, primary 节点只需更新超过半数(含自身)的节点后就返回用户成功。每次更新操作都会递增各个节 点的版本号(xzid)。当 primary 节点异常,利用 paxos 协议选举新的 primary 时,每个节点都会以自 己的版本号发起 paxos 提议,从而保证了选出的新 primary 是某个超过半数副本集合中版本号最大的副本。这个原则与 2.4.5 中描述的完全一致。值得一提的是,在 2.4.5 中分析到,新 primary 的版本 39
号未必是一个最新已提交的版本,可能是一个只更新了少于半数副本的中间态的更新版本,此时新 primary 完成与超过半数的副本同步后,这个版本的数据自动满足 quorum 的半数要求;另一方面, 新 primary 的版本可能是一个最新已􏰀交的版本,但可能会存在其他副本没有参与选举但持有一个 大于新 primary 的版本号的数据(中间态版本),和 2.4.5 分析的一样,此时这样的中间态版本数据 将被认为是脏数据,在与新 primary 进行数据同步时被 zookeeper 丢弃。

	Mola*/Armor*中的Quorum	Mola*和 Armor*系统中所有的副本管理都是基于 Quorum,即数据在多数副本上更新成功则认 为成功。Mola 系统的读取通常不关注强一致性,而提供最终一致性。对于 Armor*,可以通过读取 副本版本号的方式,按 Quorum 规则判断最新已􏰀交的版本(参考 2.4.4 )。由于每次读数据都需要 读取版本号,降低了系统系统性能,Doris*系统在读取 Armor*数据时采用了一种优化思路:由于 Doris*系统的数据是批量更新,Doris*维护了 Armor*副本的版本号,并只在每批数据更新完成后再 刷新 Armor*副本的版本号,从而大大减少了读取 Armor*副本版本号的开销。	BigPipe*中的Quorum
	Big Pipe*中的副本管理也是采用了 WARO 机制。值得一提的是,Big Pipe 利用了 zookeeper 的 高可用性解决了 WARO 在更新失败时副本的不一致的难题。当更新操作失败时,每个副本都会尝试 将自己的最后一条更新操作写入 zookeeper。但最多只有一个副本能写入成功,如果副本发现之前已 经有副本写入成功后则放弃写入,并以 zookeeper 中的记录为准与自身的数据进行同步。另一方面, 与 GFS 更新失败后新建 chunck 类似,当 Big Pipe*更新失败后,会将更新切换到另一组副本,这组 副本首先读取 zookeeper 上的最后一条记录,并从这条更新记录之后继续提供服务。	例如,一共有 3 个副本 A、B、C,某次更新操作在 A、B 上成功,各副本上的数据为(2,2, 1),此时3个副本一旦探测出异常,都会尝试向zookeeper写入最后的记录。如果A或B写入成功, 则意味着最后一次的更新操作成功,副本 C 会尝试同步到这条更新,新切换的副本组也会在数据 2 的基础上继续提供服务。如果 C 写入成功,则相当于最后一次更新失败,当副本 A、B 读到 zookeeper 上的信息后会将最后一个更新操作作为“脏数据”并回退掉最后一个更新操作,新切换的副本组也 只会在数据 1 的基础上继续提供服务。从这里不难看出,当出现更新失败时,最后一条更新操作成为类似 2.4.5 中的中间态数据。与 2.4.5 中的中间态数据的命运取决于是否能参与新 primary 的选举类似,Big Pipe*中这条中间态数据 的命运完全取决于是哪个副本抢先完成写 zookeeper 的过程。

### CAP理论与实践
