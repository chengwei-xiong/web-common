lease机制


vector clock：时钟向量，多版本数据修改

[分布式事务](http://coolshell.cn/articles/10910.html)

Gossip协议

DYNAMO

#### 分布式一致性协议的演进
![cap](./images/ds/Transaction-Across-DataCenter.jpg)
![cap](./images/ds/dt_data.jpeg)

##web设计

TOKEN-生成策略：
加上设备号相关的信息，用户ID，可以参考JWT。


那么序列值为 id << 2 | table_index; 即id向左移位2位（移位几位取决于表的个数）， 然后和表序号求或

###
分布式系统分为分布式存储系统，分布式计算系统，分布式的管理系统

分布式存储系统:

* 数据的分割
* 数据的多副本
* 数据的读写分离
* IPC通信
* 可用性保证
* 网络，硬件的异常处理

分布式计算系统:

* 分工方式
* 数据交换方式
* 支持的运算种类
* 通信处理
* 运算的状态管理
* 可用性保障

### ES
缺省情况，查询发出的请求数量为shard 的数量，可扩展
读，可以通过增加副本
复杂统计查询，瓶颈在mem/cpu/io
如果qps 要求高，主要需要靠缓存
写，通过增加机器
相对宽裕的shard数量
增加机器的时候，shard 可以自动调整到新加的机器，扩容方便

C，A
写 
by default, index operations only succeed if a quorum (>replicas/2+1) of active shards are available.
譬如3 个replica, ¾  个就可以
1个replication 的情况下，退化为one,只要有一个写成功就可以
不可用，有可能是由于master 不可用导致
读
只要一个就可以
public enum WriteConsistencyLevel {DEFAULT((byte) 0),  ONE((byte) 1),QUORUM((byte) 2),ALL((byte) 3);}
集群下有可能丢失数据
即使有primary + 2个replica, 这个是由于网络partition 导致Primary+1个replica, 更有可能导致，脑裂和数据的丢失
为了一致性，可以选write consistency level 为ALL setConsistencyLevel(WriteConsistencyLevel.ALL)

MASTER
选举master
从几个可以备选master 中选举
Master 负责shard 分配，判断shard 可用性，mapping 维护等
超过20台节点的集群，建议master 节点只做master,不放data
Master节点在网络分割等情况下，有可能不可用


* 2PC
两阶段提交，组成有事务协调者和事务发起方，业务参与者，步骤是分为事务准备阶段和事务提交阶段。
  准备阶段：由事务发起方访问事务协调者，然后事务协调者发起一个分布式事务，然后向每一个事务参与者发起（prepare commit）操作，参与者可以失败，可以成功的（在本地执行事务，写本地的redo和undo日志，但不提交）响应给协调者，协调者收到所有参与者返回的信息，决定是执行全局回滚操作，还是提交（正式的commit）.
  提交阶段：根据第一阶段的的结果发起提交阶段的操作（行为是取决于第一阶段的结果，是commit，还是rollback），参与者收到操作,开始执行本地的提交事务或者rollback操作，然后把执行的结果返回给事务协调者。
  
以上就是两阶段提交的大致过程，你会发现这个过程在理想状态下是OK的，但是在分布式系统下会有许多不确定的因素，比如，宕机，网络不稳定等；因此我们会发现其存在以下大致的问题

    1、同步阻塞问题。执行过程中，所有参与节点都是事务阻塞型的。当参与者占有公共资源时，其他第三方节点访问公共资源不得不处于阻塞状态。

    2、单点故障。由于协调者的重要性，一旦协调者发生故障。参与者会一直阻塞下去。尤其在第二阶段，协调者发生故障，那么所有的参与者还都处于锁定事务资源的状态中，而无法继续完成事务操作。（如果是协调者挂掉，可以重新选举一个协调者，但是无法解决因为协调者宕机导致的参与者处于阻塞状态的问题）

    3、数据不一致。在二阶段提交的阶段二中，当协调者向参与者发送commit请求之后，发生了局部网络异常或者在发送commit请求过程中协调者发生了故障，这回导致只有一部分参与者接受到了commit请求。而在这部分参与者接到commit请求之后就会执行commit操作。但是其他部分未接到commit请求的机器则无法执行事务提交。于是整个分布式系统便出现了数据部一致性的现象。

    4、二阶段无法解决的问题：协调者再发出commit消息之后宕机，而唯一接收到这条消息的参与者同时也宕机了。那么即使协调者通过选举协议产生了新的协调者，这条事务的状态也是不确定的，没人知道事务是否被已经提交。
    
    5、第二阶段参与者中有部分是执行成功并返回给任务协调者但是有一部分参与者执行失败，这就导致整个系统的数据不一致的问题。

* 3PC
接下来让我们来看看三阶段提交时如何解决2PC所产生的问题的，3pc的组成角色和2PC是一致的，其优化的点分为以下两点

1、引入超时机制。同时在协调者和参与者中都引入超时机制。
2、在第一阶段之前插入一个确认阶段。保证了在最后提交阶段之前各参与节点的状态是一致的。

除了引入超时机制之外，3PC把2PC的准备阶段再次一分为二，这样三阶段提交就有CanCommit、PreCommit、DoCommit三个阶段。

  CanCommit阶段：该步骤和2PC的不走类似，但是该步骤下的参与者不锁定资源和写事务日志，只是一个确认达成共识的步骤
  PreCommit阶段：协调者根据参与者的反应情况来决定是否可以记性事务的PreCommit操作。根据响应情况，有以下两种可能。
    假如协调者从所有的参与者获得的反馈都是Yes响应，那么就会执行事务的预执行。

    1. 发送预提交请求 协调者向参与者发送PreCommit请求，并进入Prepared阶段。

    2. 事务预提交 参与者接收到PreCommit请求后，会执行事务操作，并将undo和redo信息记录到事务日志中。

    3. 响应反馈 如果参与者成功的执行了事务操作，则返回ACK响应，同时开始等待最终指令。
    假如有任何一个参与者向协调者发送了No响应，或者等待超时之后，协调者都没有接到参与者的响应，那么就执行事务的中断。

    1.发送中断请求 协调者向所有参与者发送abort请求。

    2.中断事务 参与者收到来自协调者的abort请求之后（或超时之后，仍未收到协调者的请求），执行事务的中断。
  DoCommit阶段：
  该阶段进行真正的事务提交，也可以分为以下两种情况。
  执行提交
  
      1.发送提交请求 协调接收到参与者发送的ACK响应，那么他将从预提交状态进入到提交状态。并向所有参与者发送doCommit请求。

      2.事务提交 参与者接收到doCommit请求之后，执行正式的事务提交。并在完成事务提交之后释放所有事务资源。

      3.响应反馈 事务提交完之后，向协调者发送Ack响应。

      4.完成事务 协调者接收到所有参与者的ack响应之后，完成事务。
  中断事务 协调者没有接收到参与者发送的ACK响应（可能是接受者发送的不是ACK响应，也可能响应超时），那么就会执行中断事务。

      1.发送中断请求 协调者向所有参与者发送abort请求

      2.事务回滚 参与者接收到abort请求之后，利用其在阶段二记录的undo信息来执行事务的回滚操作，并在完成回滚之后释放所有的事务资源。

      3.反馈结果 参与者完成事务回滚之后，向协调者发送ACK消息

      4.中断事务 协调者接收到参与者反馈的ACK消息之后，执行事务的中断。
  在doCommit阶段，如果参与者无法及时接收到来自协调者的doCommit或者rebort请求时，会在等待超时之后，会继续进行事务的提交。（其实这个应该是基于概率来决定的，当进入第三阶段时，说明参与者在第二阶段已经收到了PreCommit请求，那么协调者产生PreCommit请求的前提条件是他在第二阶段开始之前，收到所有参与者的CanCommit响应都是Yes。（一旦参与者收到了PreCommit，意味他知道大家其实都同意修改了）所以，一句话概括就是，当进入第三阶段时，由于网络超时等原因，虽然参与者没有收到commit或者abort响应，但是他有理由相信：成功提交的几率很大。 ）

其实3PC还是无法解决参与者数据不一致的问题，只是将不一致的问题发生的概率降低了
![3PC的不确定因素](./images/ds/Three-phase_commit_status.png)

* PAXOS

* Raft

Raft是工程化中非常有名且有效的分布式一致性算法，本次关注：Leader选举和日志的部分（对应Lab 2A与2B），下节关注：持久化、客户端行为和快照（Lab 2C和Lab 3）

Raft是Paxos算法的简化版本，基于自动机复制(state machine replcation)，即所有备份节点都执行相同的命令序列，以求得到相同的结果。核心问题就是要处理脑裂的问题，实现的基础在于保证“大多数”节点还是存活的，这里的“大多数”（quorm）指总数中超过半数。

Raft使用日志作为复制的单位载体（仅利用KV DB来维护状态并不够），思路是编号定序与方便存储，编号定序保证复制命令执行顺序性，存储保证消息可靠性。

Raft设计的两个难点：

选择新leader
保证日志执行顺序，即使出现各种失效
Lab2A就是要实现一个选主算法。

首先考虑为何要选出leader？可以简化设计，保证其他节点执行按相同顺序执行相同命令。Raft使用term概念标记leader顺序，一个term中最多一个leader（可能没有leader），帮助其他节点找到最新的leader。

何时开始选主？节点发现收到主节点通信时，增加自己的term值，发起选举。注意这样可能导致不必要的选举过程，但是安全；也可能出现原leader还存活并且自认还是leader的情况。旧leader仅会影响“小部分”节点，大多数不会执行它的指令。

如何保证只有最多一个leader获胜？所有节点每轮只需投票一次，回应第一个询问的节点。获得“大多数“”节点投票信息的leader获胜。其他节点通过接收主节点的AppendEntries和心跳信息得知leader的获胜与存活。可能出现没有leader胜出的情况【活锁问题】，此时会超时并重新发起选举（term值递增）。

如何选择超时时间？至少几个心跳时间，节点随机选择长度，但要足够完成一轮选举。

选主算法的工程实现要点

节点需要维护的字段：

参数	解释
currentTerm	服务器最后一次知道的任期号（初始化为 0，持续递增），需要持久化
votedFor	在当前term获得选票的候选人 Id，需要持久化
log[]	日志条目集；每一个条目包含一个用户状态机执行的指令，和收到时的任期号，需要持久化
--	--
state	节点状态（Leader，Candidate，Follower）三种之一
commitIndex	已知的最大的已经被提交的日志条目的索引值
lastApplied	最后被应用到状态机的日志条目索引值（初始化为 0，持续递增）
--	--
nextIndex[]	对于每一个服务器，需要发送给他的下一个日志条目的索引值（初始化为领导人最后索引值加一），leader有用
matchIndex[]	对于每一个服务器，已经复制给他的日志的最高索引值，leader有用
节点可以发出两个RPC：

AppendLogEntries
平时的日志同步RPC，同时有心跳功能。

如果接收到的 RPC 请求中，term > currentTerm ，那么就令 currentTerm 等于 term，并切换状态为跟随者
Args参数	解释
term	领导人的任期号
leaderId	领导人的 Id，以便于跟随者重定向请求
prevLogIndex	新的日志条目紧随之前的索引值
prevLogTerm	prevLogIndex条目的任期值
entries[]	需要存储当然日志条目（表示心跳时为空；一次性发送多个是为了提高效率）
leaderCommit	领导人已经提交的日志的索引值
--	--
Reply返回值	解释
term	当前的任期号，用于领导人去更新自己
success	跟随者包含了匹配上 prevLogIndex 和 prevLogTerm 的日志时为真
RequestVoteFor
由候选人负责调用用来征集选票。

如果 term < 接收节点的currentTerm，则设置term并返回 voteGranted = false表示忽略这个选票
如果接收节点的 votedFor 为空或者等于 candidateId，并且lastLogXXX信息与自己的一样新，则继续返回投票
如果接收到的 RPC 请求中，term > currentTerm ，那么就令 currentTerm 等于 term，并切换状态为跟随者
Args参数	解释
term	候选人的任期号
candidateId	请求选票的候选人的 Id
lastLogIndex	候选人的最后日志条目的索引值
lastLogTerm	候选人最后日志条目的任期号
--	--
Reply返回值	解释
term	当前任期号，以便于候选人去更新自己的任期号
voteGranted	候选人赢得了此张选票时为真
节点线程模型

所有节点Follower启动一个检测线程，如果在超过选举超时时间的情况之前都没有收到Leader的心跳，或者是候选人请求投票的，就自己变成Candidate
变为Candidate启动一个选举线程，
自增当前的任期号（currentTerm）
给自己投票
重置选举超时计时器，选举超时时间是从一个固定的区间（例如 150-300毫秒）随机选择
发送请求投票的 RPC 给其他所有服务器
如果接收到大多数服务器的选票，那么就变成领导人
如果接收到的 RPC 请求中，term > currentTerm ，那么就令 currentTerm 等于 term，并切换状态为跟随者。注意在等待投票的时候，candidate可能会从其他节点接收到声明它是leader的AppendLogEntries RPC
成为Leader后启动一个心跳线程，定期发送心跳信息（100毫秒）

* 最终一致性协议

	* 补偿模式：将消息的不可靠和处理失败的问题由单独的补偿方式来解决，需要提供补偿的接口来做，事务数据的补偿操作。
	* TCC(Try/Confirm/Cancel): 和两阶段提交有点类似，参与者提供的事务模型为（Try/Confirm/Cancel）
		![](./images/ds/tcc.jpg)
    * 可靠事件模式（非事务消息、事务消息）：

		* 非事务消息: 需要自身维护消息日志表，面临的问题是消息的发送方重复发送和消息的消费方的幂等设计

	 		```
        操作数据库成功，向MQ中投递消息也成功，皆大欢喜；
        操作数据库失败，不会向MQ中投递消息了；
        操作数据库成功，但是向MQ中投递消息时失败，向外抛出了异常，刚刚执行的更新数据库的操作将被回滚。
        	```
        * 事务消息: 采用一些支持分布式事务的消息中间件(RocketMQ,Notify）
        
        	```
        发送消息，并记录该消息的唯一序列,有一个消息状态协同系统，记录消息的状态。
		开始本地事务，准备commit或者rollback
		根据序列更新消息记录表中的状态，定期扫描消息状态协同系统的消息记录表，来确认消息是成功还是失败，来处理回滚。
			```
       ![如图](./images/ds/transtion_msg.jpg)
  
  + Quorum W+R>N：抽屉原理，数据一致性的另一种解决方案
  
    最后我还想提一下Amazon Dynamo的NWR模型。这个NWR模型把CAP的选择权交给了用户，让用户自己的选择你的CAP中的哪两个。

    所谓NWR模型。N代表N个备份，W代表要写入至少W份才认为成功，R表示至少读取R个备份。配置的时候要求W+R > N。 因为W+R > N， 所以 R > N-W 这个是什么意思呢？就是读取的份数一定要比总备份数减去确保写成功的倍数的差值要大。

    也就是说，每次读取，都至少读取到一个最新的版本。从而不会读到一份旧数据。当我们需要高可写的环境的时候，我们可以配置W = 1 如果N=3 那么R = 3。 这个时候只要写任何节点成功就认为成功，但是读的时候必须从所有的节点都读出数据。如果我们要求读的高效率，我们可以配置 W=N R=1。这个时候任何一个节点读成功就认为成功，但是写的时候必须写所有三个节点成功才认为成功。

    NWR模型的一些设置会造成脏数据的问题，因为这很明显不是像Paxos一样是一个强一致的东西，所以，可能每次的读写操作都不在同一个结点上，于是会出现一些结点上的数据并不是最新版本，但却进行了最新的操作。

    所以，Amazon Dynamo引了数据版本的设计。也就是说，如果你读出来数据的版本是v1，当你计算完成后要回填数据后，却发现数据的版本号已经被人更新成了v2，那么服务器就会拒绝你。版本这个事就像“乐观锁”一样。

    但是，对于分布式和NWR模型来说，版本也会有恶梦的时候——就是版本冲的问题，比如：我们设置了N=3 W=1，如果A结点上接受了一个值，版本由v1 -> v2，但还没有来得及同步到结点B上（异步的，应该W=1，写一份就算成功），B结点上还是v1版本，此时，B结点接到写请求，按道理来说，他需要拒绝掉，但是他一方面并不知道别的结点已经被更新到v2，另一方面他也无法拒绝，因为W=1，所以写一分就成功了。于是，出现了严重的版本冲突。

    Amazon的Dynamo把版本冲突这个问题巧妙地回避掉了——版本冲这个事交给用户自己来处理。

    于是，Dynamo引入了Vector Clock（矢量钟？!）这个设计。这个设计让每个结点各自记录自己的版本信息，也就是说，对于同一个数据，需要记录两个事：1）谁更新的我，2）我的版本号是什么。

    下面，我们来看一个操作序列：

    1）一个写请求，第一次被节点A处理了。节点A会增加一个版本信息(A，1)。我们把这个时候的数据记做D1(A，1)。 然后另外一个对同样key的请求还是被A处理了于是有D2(A，2)。这个时候，D2是可以覆盖D1的，不会有冲突产生。

    2）现在我们假设D2传播到了所有节点(B和C)，B和C收到的数据不是从客户产生的，而是别人复制给他们的，所以他们不产生新的版本信息，所以现在B和C所持有的数据还是D2(A，2)。于是A，B，C上的数据及其版本号都是一样的。

    3）如果我们有一个新的写请求到了B结点上，于是B结点生成数据D3(A,2; B,1)，意思是：数据D全局版本号为3，A升了两新，B升了一次。这不就是所谓的代码版本的log么？

    4）如果D3没有传播到C的时候又一个请求被C处理了，于是，以C结点上的数据是D4(A,2; C,1)。

    5）好，最精彩的事情来了：如果这个时候来了一个读请求，我们要记得，我们的W=1 那么R=N=3，所以R会从所有三个节点上读，此时，他会读到三个版本：

        A结点：D2(A,2)
        B结点：D3(A,2;  B,1);
        C结点：D4(A,2;  C,1)
    6）这个时候可以判断出，D2已经是旧版本（已经包含在D3/D4中），可以舍弃。
    7）但是D3和D4是明显的版本冲突。于是，交给调用方自己去做版本冲突处理。就像源代码版本管理一样。

    很明显，上述的Dynamo的配置用的是CAP里的A和P。


#### CAP
1，P是分布式系统的起点，全称：network partition，常指拔网线和宕机，因为这两种现象，同伴是无法区分的，所以归为一类。分布式系统下，P是大概率事件，假设服务器开一年必宕机一次，则365台的集群必然天天宕机。数据分存这365台之上，则数据必然天天不完整。
你设计的分布式系统能容忍吗？绝大多数是不能，不能则

2，复制，是解决这一问题的标准做法

3，复制，引发了C

4，C引发了A

一致性（数据的一致性，ACID），高可用(快速的响应，即使是失败也能快速的响应)，分区容忍性，
![CAP](./images/ds/cap.png)

#### BASE

BASE理论

BASE是Basically Available（基本可用）、Soft state（软状态）和Eventually consistent（最终一致性）三个短语的简写，是由来自eBay的架构师Dan Pritchett在其文章BASE: An Acid Alternative注 中第一次明确提出的。BASE是对CAP中一致性和可用性权衡的结果，其来源于对大规模互联网系统分布式实践的总结，是基于CAP定理逐步演化而来的，其核心思想是即使无法做到强一致性(Strong consistency)，但每个应用都可以根据自身的业务特点，采用适当的方式来使系统达到最终一致性(Eventual consistency)。接下来我们着重对BASE中的三要素进行详细讲解。

* 基本可用
基本可用是指分布式系统在出现不可预知故障的时候，允许损失部分可用性——但请注意，这绝不等价于系统不可用。以下两个就是“基本可用”的典型例子。
响应时间上的损失：正常情况下，一个在线搜索引擎需要在0.5秒之内返回给用户相应的查询结果，但由于出现故障（比如系统部分机房发生断电或断网故障），查询结果的响应时间增加到了1～2秒。
功能上的损失：正常情况下，在一个电子商务网站上进行购物，消费者几乎能够顺利地完成每一笔订单，但是在一些节日大促购物高峰的时候，由于消费者的购物行为激增，为了保护购物系统的稳定性，部分消费者可能会被引导到一个降级页面。

* 弱状态
弱状态也称为软状态，和硬状态相对，是指允许系统中的数据存在中间状态，并认为该中间状态的存在不会影响系统的整体可用性，即允许系统在不同节点的数据副本之间进行数据同步的过程存在延时。
* 最终一致性
最终一致性强调的是系统中所有的数据副本，在经过一段时间的同步后，最终能够达到一个一致的状态。因此，最终一致性的本质是需要系统保证最终数据能够达到一致，而不需要实时保证系统数据的强一致性。
亚马逊首席技术官Werner Vogels在于2008年发表的一篇经典文章Eventually Consistent-
Revisited中，对最终一致性进行了非常详细的介绍。他认为最终一致性是一种特殊的弱一致性：系统能够保证在没有其他新的更新操作的情况下，数据最终一定能够达到一致的状态，因此所有客户端对系统的数据访问都能够获取到最新的值。同时，在没有发生故障的前提下，数据达到一致状态的时间延迟，取决于网络延迟、系统负载和数据复制方案设计等因素。
在实际工程实践中，最终一致性存在以下五类主要变种。
* 因果一致性（Causal consistency）
因果一致性是指，如果进程A在更新完某个数据项后通知了进程B，那么进程B之后对该数据项的访问都应该能够获取到进程A更新后的最新值，并且如果进程B要对该数据项进行更新操作的话，务必基于进程A更新后的最新值，即不能发生丢失更新情况。与此同时，与进程A无因果关系的进程C的数据访问则没有这样的限制。
* 读己之所写（Read your writes）
读己之所写是指，进程A更新一个数据项之后，它自己总是能够访问到更新过的最新值，而不会看到旧值。也就是说，对于单个数据获取者来说，其读取到的数据，一定不会比自己上次写入的值旧。因此，读己之所写也可以看作是一种特殊的因果一致性。
* 会话一致性（Session consistency）
会话一致性将对系统数据的访问过程框定在了一个会话当中：系统能保证在同一个有效的会话中实现“读己之所写”的一致性，也就是说，执行更能操作之后，客户端能够在同一个会话中始终读取到该数据项的最新值。
* 单调读一致性（Monotonic read consistency）
单调读一致性是指如果一个进程从系统中读取出一个数据项的某个值后，那么系统对于该进程后续的任何数据访问都不应该返回更旧的值。
* 单调写一致性（Monotonic write consistency）
单调写一致性是指，一个系统需要能够保证来自同一个进程的写操作被顺序地执行。

以上就是最终一致性的五类常见的变种，在实际系统实践中，可以将其中的若干个变种互相结合起来，以构建一个具有最终一致性特性的分布式系统。事实上，最终一致性并不是只有那些大型分布式系统才涉及的特性，许多现代的关系型数据库都采用了最终一致性模型。在现代关系型数据库中，大多都会采用同步和异步方式来实现主备数据复制技术。在同步方式中，数据的复制过程通常是更新事务的一部分，因此在事务完成后，主备数据库的数据就会达到一致。而在异步方式中，备库的更新往往会存在延时，这取决于事务日志在主备数据库之间传输的时间长短，如果传输时间过长或者甚至在日志传输过程中出现异常导致无法及时将事务应用到备库上，那么很显然，从备库中读取的数据将是旧的，因此就出现了数据不一致的情况。当然，无论是采用多次重试还是人为数据订正，关系型数据库还是能够保证最终数据达到一致——这就是系统提供最终一致性保证的经典案例。

总的来说，BASE理论面向的是大型高可用可扩展的分布式系统，和传统事务的ACID特性是相反的，它完全不同于ACID的强一致性模型，而是提出通过牺牲强一致性来获得可用性，并允许数据在一段时间内是不一致的，但最终达到一致状态。但同时，在实际的分布式场景中，不同业务单元和组件对数据一致性的要求是不同的，因此在具体的分布式系统架构设计过程中，ACID特性与BASE理论往往又会结合在一起使用。
